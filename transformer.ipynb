{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "### 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils import data as Data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练设备配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512 #嵌入向量的维度\n",
    "max_len = 100 #句子的最大长度\n",
    "d_ff = 2048 #前馈神经网络的隐层维度，为嵌入向量维度的4倍\n",
    "d_k = d_v = 64 #注意力机制中Q、K、V的维度，Q和K的维度为d_k,V的维度为d_v\n",
    "n_layers = 6 #编码器和解码器的层数\n",
    "n_heads = 8 #多头注意力机制的头数\n",
    "p_drop = 0.1 #dropout概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义\n",
    "\n",
    "### Mask\n",
    "\n",
    "#### Pad Mask for Attention\n",
    "\n",
    "由于在数据中使用了padding进行填充，而不希望pad被加入到注意力中进行计算，可以使用Attention Pad Mask，其作用是确保模型在计算注意力分数时,任何Query不会去关注**Key中的padding**位置\n",
    "\n",
    "这里假设\\<pad\\>在字典中的索引为0，那么当输入为0时，返回True，否则返回False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    padding部分的attention mask\n",
    "    防止Query对Key中的padding部分计算attention\n",
    "    \n",
    "    parameters:\n",
    "    seq_q: [batch_size, len_q]\n",
    "    seq_k: [batch_size, len_k]\n",
    "\n",
    "    return:\n",
    "    mask: [batch_size, len_q, len_k]\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsequent Mask for Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    在自回归时防止后面的信息影响前面的信息\n",
    "\n",
    "    parameters:\n",
    "    seq: [batch_size, seq_len]\n",
    "\n",
    "    return:\n",
    "    subsequent_mask: [batch_size, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
    "    # np.triu()返回矩阵的上三角部分, k=1表示对角线之上的元素,k越大，则上三角范围越小\n",
    "    # 与之完全相反的是np.tril()函数，返回矩阵的下三角部分\n",
    "    # subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    # subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape, dtype=torch.uint8, device=seq.device), diagonal=1)\n",
    "    return subsequent_mask\n",
    "# attn_shape = [5, 5, 5]\n",
    "# torch.from_numpy(np.triu(np.ones(attn_shape), k=1)).byte()== torch.triu(torch.ones(attn_shape, dtype=torch.uint8), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "绝对位置编码\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})\n",
    "$$\n",
    "\n",
    "\n",
    "- 唯一性：每个位置需要独特编码，不会出现编码重复问题\n",
    "    - 不同维度使用不同的周期长度\n",
    "    - 多个维度的组合提供了足够的唯一性\n",
    "    - 每个位置都有唯一的编码模式，\n",
    "- 平滑性：相近位置应有相似编码\n",
    "    - $10000^{2i/d_{model}}$​ 的作用：\n",
    "        - 这个项会随着维度i的增加而增大\n",
    "        - 导致不同维度上的正弦/余弦函数有不同的频率\n",
    "        - 低维度对应高频信号，对近距离位置敏感\n",
    "        - 高维度对应低频信号，可以捕捉长距离依赖\n",
    "- 有界性：编码值要在固定范围内，正弦和余弦函数的值域在\\[-1,1\\]之间，适合作为神经网络的输入\n",
    "- 可推广性：能处理任意长度序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=512, p_drop=.1, max_len=1024):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=p_drop)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1) # [max_len, 1]\n",
    "        \n",
    "        # div_term = 1 / (10000^(2i/sqrt(d_model)))\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-torch.log(torch.Tensor([10000])) / d_model / 2))\n",
    "        # position: [max_len, 1], div_term: [d_model/2]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # [max_len, d_model] -> [1, max_len, d_model] \n",
    "\n",
    "        # 存疑：为什么不直接用unsqueeze(1)而是要用unsqueeze(0).transpose(0, 1)\n",
    "        # pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 能够申请缓冲区的张量，可以通过调用register_buffer()方法将其注册为模型的一部分,且不会参与到梯度的计算\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 存疑：为什么要加上pe[:, :x.size(1)]，而不是直接加上pe\n",
    "        x = x + self.pe[:, :x.size(1),:] # [batch_size, seq_len, d_model]\n",
    "        return self.dropout(x)\n",
    "\n",
    "x = torch.ones(4, 6, 6)\n",
    "net = PositionalEncoding(d_model=6)\n",
    "print(net(x).shape)\n",
    "\n",
    "\n",
    "# x = torch.randn(10, 3, 4)\n",
    "# # print(\"x:\", x) \n",
    "\n",
    "# pe = torch.ones(10, 1, 4)\n",
    "# # print(\"x + pe[:, :x.size(1)]:\", x + pe[:, :x.size(1)])  # torch.Size([5, 1, 4])\n",
    "# # print(\"x + pe:\", x + pe) \n",
    "# print(x + pe[:, :x.size(1)] == x + pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network\n",
    "\n",
    "在Transformer中，每个Encoder和Decoder都有一个全连接前馈神经网络来添加非线性特征\n",
    "$$\n",
    "FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=4*d_model, p_drop=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        residual = x\n",
    "        # x : [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff]\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        # x : [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_norm(x + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "FeedForwardNetwork                       [3, 4, 512]               --\n",
      "├─Linear: 1-1                            [3, 4, 2048]              1,050,624\n",
      "├─ReLU: 1-2                              [3, 4, 2048]              --\n",
      "├─Linear: 1-3                            [3, 4, 512]               1,049,088\n",
      "├─Dropout: 1-4                           [3, 4, 512]               --\n",
      "├─LayerNorm: 1-5                         [3, 4, 512]               1,024\n",
      "==========================================================================================\n",
      "Total params: 2,100,736\n",
      "Trainable params: 2,100,736\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 6.30\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.29\n",
      "Params size (MB): 8.40\n",
      "Estimated Total Size (MB): 8.72\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "net = FeedForwardNetwork()\n",
    "print(summary(net, (3, 4 ,512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention机制是Transformer的核心，它可以将输入序列的不同位置的信息进行加权求和，从而实现对不同位置的关注\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "单头注意力机制，输入包括三个部分：查询Q，键K，值V，计算公式如下：\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "![Scaled Dot-Product Attention](https://bex-image.oss-cn-hangzhou.aliyuncs.com/img/202412211526401.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k=64, d_v=64):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q: [batch_size, n_heads, len_q, d_k]\n",
    "        # K: [batch_size, n_heads, len_k, d_k]\n",
    "        # V: [batch_size, n_heads, len_v, d_v]\n",
    "        # attn_mask: [batch_size, n_heads, len_q, len_k]\n",
    "        \n",
    "        # Q * K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.d_k) # [batch_size, n_heads, len_q, len_k]\n",
    "        # masked_fill_能把传进来的Mask为True的地方全都填充上某个值,\n",
    "        # 这里需要用一个很大的负数来保证，从而使softmax后的值接近于0\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "\n",
    "        attn = nn.Softmax(dim=-1)(scores) # [batch_size, n_heads, len_q, len_k]\n",
    "\n",
    "        prob = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n",
    "        return prob, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention\n",
    "\n",
    "多头注意力是指通过多个不同的注意力头来获取不同的特征，然后将这些特征拼接起来，通过线性变换得到最终的输出\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O\n",
    "$$\n",
    "$$\n",
    "\\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "![Multi-Head Attention](https://bex-image.oss-cn-hangzhou.aliyuncs.com/img/202412211529171.png)\n",
    "\n",
    "> 虽然新版本已经有reshape函数可以用了, 但是仍然不要忘记, transpose后如果接permute或者view必须要加contiguous, 这是数据真实存储连续与否的问题,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_k=64, d_v=64, n_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # 虽然在原结构图中使用了多个线性层，但是这里使用一个线性层进行一次性计算\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
    "        self.W_O = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, input_Q, input_K, input_V, attn_mask):\n",
    "        '''\n",
    "        To make sure multihead attention can be used both in encoder and decoder, \n",
    "        we use Q, K, V respectively.\n",
    "        \n",
    "        parameters:\n",
    "        input_Q: [batch_size, len_q, d_model]\n",
    "        input_K: [batch_size, len_k, d_model]\n",
    "        input_V: [batch_size, len_v, d_model]\n",
    "        attn_mask: [batch_size, len_q, len_k]\n",
    "        '''\n",
    "        residual, batch_size = input_Q, input_Q.size(0)\n",
    "\n",
    "        # [batch_size, len_q, d_model] -- matmul W_Q -> [batch_size, len_q, d_k * n_heads] -- reshape  -> [batch_size, n_heads, len_q, d_k]\n",
    "        Q = self.W_Q(input_Q).reshape(batch_size, self.n_heads, -1, self.d_k) # [batch_size, n_heads, len_q, d_k]\n",
    "        K = self.W_K(input_K).reshape(batch_size, self.n_heads, -1, self.d_k) # [batch_size, n_heads, len_k, d_k]\n",
    "        V = self.W_V(input_V).reshape(batch_size, self.n_heads, -1, self.d_v) # [batch_size, n_heads, len_v, d_v]\n",
    "\n",
    "        # attn_mask: [batch_size, len_q, len_k] -- unsqueeze(1) -> [batch_size, 1, len_q, len_k] -- repeat -> [batch_size, n_heads, len_q, len_k]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1) \n",
    "        \n",
    "        # prob: [batch_size, n_heads, len_q, d_v]\n",
    "        # attn: [batch_size, n_heads, len_q, len_k]\n",
    "        prob, attn = ScaledDotProductAttention(self.d_k, self.d_v)(Q, K, V, attn_mask)\n",
    "\n",
    "        prob = prob.transpose(1, 2).contiguous() # [batch_size, len_q, n_heads, d_v]\n",
    "        # prob = prob.view(prob.size(0), -1, self.n_heads * self.d_v).contiguous() # [batch_size, len_q, n_heads * d_v]\n",
    "        prob = prob.reshape(prob.size(0), -1, self.n_heads * self.d_v) # [batch_size, len_q, n_heads * d_v]\n",
    "        output = self.W_O(prob) # [batch, len_q, d_model]\n",
    "\n",
    "        return self.layer_norm(residual + output), attn\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder\n",
    "![image-20241221153148582](https://bex-image.oss-cn-hangzhou.aliyuncs.com/img/202412211531617.png)\n",
    "\n",
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderlayer(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, d_k=64, d_v=64, n_heads=8, p_drop=0.1):\n",
    "        super(Encoderlayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model=d_model, d_k=d_k, d_v=d_v, n_heads=n_heads)\n",
    "        self.feed_forward_network = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, p_drop=p_drop)\n",
    "    def forward(self, encoder_input, encoder_pad_mask):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "        encoder_input: [batch_size, seq_len, d_model]\n",
    "        encoder_pad_mask: [batch_size, seq_len, seq_len]\n",
    "\n",
    "        return:\n",
    "        encoder_output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # multi_head_attention & add & norm\n",
    "        # multi_head_attention(Q, K, V, attn_mask)\n",
    "        encoder_output, attn = self.multi_head_attention(encoder_input, encoder_input, encoder_input, encoder_pad_mask)\n",
    "        # feed_forward_network & add & norm\n",
    "        encoder_output = self.feed_forward_network(encoder_output)\n",
    "        return encoder_output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model=512, max_len=1024, d_ff=2048, d_k=64, d_v=64, n_layers=6, n_heads=8, p_drop=.1, source_vocab_size=1000):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_embedding = nn.Embedding(source_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, p_drop, max_len)\n",
    "        self.encoder_layers = nn.ModuleList([Encoderlayer(d_model=d_model, d_ff=d_ff, d_k=d_k, d_v=d_v, n_heads=n_heads, p_drop=p_drop) for _ in range(n_layers)])\n",
    "    def forward(self, encoder_input):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "        encoder_input: [batch_size, seq_len]\n",
    "\n",
    "        return:\n",
    "\n",
    "        \"\"\"\n",
    "        # 将token转换为词嵌入向量\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "        encoder_output = self.input_embedding(encoder_input)\n",
    "\n",
    "        # 将向量加上位置编码\n",
    "        # positional_encoding的输入形状: [batch_size, seq_len, d_model]\n",
    "        # positional_encoding的输出形状: [batch_size, seq_len, d_model]\n",
    "        encoder_output = self.positional_encoding(encoder_output)\n",
    "\n",
    "        # 获取padding mask\n",
    "        encoder_pad_mask = get_attn_pad_mask(encoder_input, encoder_input)\n",
    "\n",
    "        # 保存每一层的attention\n",
    "        attns = []\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output, attn = encoder_layer(encoder_output, encoder_pad_mask)\n",
    "            attns.append(attn)\n",
    "        return encoder_output, attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderlayer(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048, d_k=64, d_v=64, n_heads=8, p_drop=0.1):\n",
    "        super(Decoderlayer, self).__init__()\n",
    "        self.decoder_self_attention = MultiHeadAttention(d_model=d_model, d_k=d_k, d_v=d_v, n_heads=n_heads)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model=d_model, d_k=d_k, d_v=d_v, n_heads=n_heads)\n",
    "        self.feed_forward_network = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, p_drop=p_drop)\n",
    "    def forward(self, decoder_input, encoder_output, decoder_self_mask, decoder_encoder_pad_mask):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "        decoder_input: [batch_size, seq_len, d_model]\n",
    "        encoder_output: [batch_size, seq_len, d_model]\n",
    "        decoder_self_mask: [batch_size, seq_len, seq_len]\n",
    "        decoder_pad_mask: [batch_size, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # masked multihead attention & add & norm\n",
    "        # decoder_self_attention(Q, K, V, attn_mask)\n",
    "        # Q, K, V都来自decoder_input\n",
    "        # decoder_output: [batch_size, seq_len, d_model]\n",
    "        # decoder_self_attn: [batch_size, n_heads, seq_len, seq_len]\n",
    "        decoder_output, decoder_self_attn = self.decoder_self_attention(decoder_input, decoder_input, decoder_input, decoder_self_mask)\n",
    "        \n",
    "        # multihead attention & add & norm\n",
    "        # decoder_encoder_attention(Q, K, V, attn_mask)\n",
    "        # Q来自decoder_output, K, V来自encoder_output\n",
    "        # decoder_output: [batch_size, seq_len, d_model]\n",
    "        # decoder_encoder_attn: [batch_size, n_heads, seq_len, seq_len]\n",
    "        decoder_output, decoder_encoder_attn = self.encoder_decoder_attention(decoder_output, encoder_output, encoder_output, decoder_encoder_pad_mask)\n",
    "        decoder_output = self.feed_forward_network(decoder_output)\n",
    "\n",
    "        return decoder_output, decoder_self_attn, decoder_encoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model=512, max_len=1024, d_ff=2048, d_k=64, d_v=64, n_layers=6, n_heads=8, p_drop=.1, target_vocab_size=1000):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, p_drop, max_len)\n",
    "        self.decoder_layers = nn.ModuleList([Decoderlayer(d_model=d_model, d_ff=d_ff, d_k=d_k, d_v=d_v, n_heads=n_heads, p_drop=p_drop) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, decoder_input, encoder_input, encoder_output):\n",
    "        \"\"\"\n",
    "        parameters:\n",
    "        decoder_input: [batch_size, seq_len]\n",
    "        encoder_input: [batch_size, seq_len]\n",
    "        encoder_output: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # 将token转换为词嵌入向量\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, d_model]\n",
    "        decoder_output = self.target_embedding(decoder_input) \n",
    "        # 将向量加上位置编码\n",
    "        decoder_output = self.positional_encoding(decoder_output) # [batch_size, seq_len, d_model]\n",
    "\n",
    "        decoder_subsequent_mask = get_attn_subsequent_mask(decoder_input)\n",
    "        decoder_self_pad_mask = get_attn_pad_mask(decoder_input, decoder_input)\n",
    "\n",
    "        decoder_encoder_pad_mask = get_attn_pad_mask(decoder_input, encoder_input)\n",
    "\n",
    "        # 0为阈值，大于0的地方为1，小于0的地方为0\n",
    "        # 从而使True+True=2>1 -> True, False+True=1>1 -> True, False+False=0<1 -> False\n",
    "        decoder_self_mask = torch.gt(decoder_self_pad_mask + decoder_subsequent_mask, 0)\n",
    "\n",
    "        decoder_self_attns, decoder_encoder_attns = [], []\n",
    "\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            # decoder_output: [batch_size, seq_len, d_model]\n",
    "            decoder_output, decoder_self_attn, decoder_encoder_attn = decoder_layer(decoder_output, encoder_output, decoder_self_mask, decoder_encoder_pad_mask)\n",
    "            \n",
    "            decoder_encoder_attns.append(decoder_encoder_attn)\n",
    "            decoder_self_attns.append(decoder_self_attn)\n",
    "        \n",
    "        return decoder_output, decoder_self_attns, decoder_self_attns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=512, max_len=1024, d_ff=2048, d_k=64, d_v=64, n_layers=6, n_heads=8, p_drop=0.1, source_vocab_size=1000, target_vocab_size=1000):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model=d_model, max_len=max_len, d_ff=d_ff, d_k=d_k, d_v=d_v, n_layers=n_layers, n_heads=n_heads, p_drop=p_drop, source_vocab_size=source_vocab_size)\n",
    "        self.decoder = Decoder(d_model=d_model, max_len=max_len, d_ff=d_ff, d_k=d_k, d_v=d_v, n_layers=n_layers, n_heads=n_heads, p_drop=p_drop, target_vocab_size=target_vocab_size)\n",
    "        self.projection = nn.Linear(d_model, target_vocab_size, bias=False)\n",
    "\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        \"\"\"\n",
    "        encoder_input: [batch_size, seq_len]\n",
    "        decoder_input: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # encoder_output: [batch_size, seq_len, d_model]\n",
    "        # encoder_self_attns: [n_layers, batch_size, n_heads, seq_len, seq_len]\n",
    "        encoder_output, encoder_self_attns = self.encoder(encoder_input)\n",
    "\n",
    "        # decoder_output: [batch_size, seq_len, d_model]\n",
    "        # decoder_self_attns: [n_layers, batch_size, n_heads, seq_len, seq_len]\n",
    "        # decoder_encoder_attns: [n_layers, batch_size, n_heads, seq_len, seq_len]\n",
    "        decoder_output, decoder_self_attns, decoder_encoder_attns = self.decoder(decoder_input, encoder_input, encoder_output)\n",
    "        \n",
    "        # output: [batch_size, seq_len, target_vocab_size]\n",
    "        output = self.projection(decoder_output)\n",
    "\n",
    "        # output: [batch_size, seq_len, target_vocab_size] -- reshape --> [batch * seq_len, target_vocab_size]\n",
    "        return output.reshape(-1, output.size(-1)), encoder_self_attns, decoder_self_attns, decoder_encoder_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Transformer                                   [1600, 1000]              --\n",
       "├─Encoder: 1-1                                [32, 50, 512]             --\n",
       "│    └─Embedding: 2-1                         [32, 50, 512]             512,000\n",
       "│    └─PositionalEncoding: 2-2                [32, 50, 512]             --\n",
       "│    │    └─Dropout: 3-1                      [32, 50, 512]             --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─Encoderlayer: 3-2                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-1      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-1             [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-2             [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-3             [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-4             [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-5          [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-2      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-6             [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-7               [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-8             [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-9            [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-10         [32, 50, 512]             1,024\n",
       "│    │    └─Encoderlayer: 3-3                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-3      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-11            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-12            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-13            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-14            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-15         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-4      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-16            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-17              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-18            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-19           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-20         [32, 50, 512]             1,024\n",
       "│    │    └─Encoderlayer: 3-4                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-5      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-21            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-22            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-23            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-24            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-25         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-6      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-26            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-27              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-28            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-29           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-30         [32, 50, 512]             1,024\n",
       "│    │    └─Encoderlayer: 3-5                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-7      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-31            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-32            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-33            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-34            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-35         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-8      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-36            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-37              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-38            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-39           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-40         [32, 50, 512]             1,024\n",
       "│    │    └─Encoderlayer: 3-6                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-9      [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-41            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-42            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-43            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-44            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-45         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-10     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-46            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-47              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-48            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-49           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-50         [32, 50, 512]             1,024\n",
       "│    │    └─Encoderlayer: 3-7                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-11     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-51            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-52            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-53            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-54            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-55         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-12     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-56            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-57              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-58            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-59           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-60         [32, 50, 512]             1,024\n",
       "├─Decoder: 1-2                                [32, 50, 512]             --\n",
       "│    └─Embedding: 2-4                         [32, 50, 512]             512,000\n",
       "│    └─PositionalEncoding: 2-5                [32, 50, 512]             --\n",
       "│    │    └─Dropout: 3-8                      [32, 50, 512]             --\n",
       "│    └─ModuleList: 2-6                        --                        --\n",
       "│    │    └─Decoderlayer: 3-9                 [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-13     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-61            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-62            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-63            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-64            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-65         [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-14     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-66            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-67            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-68            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-69            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-70         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-15     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-71            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-72              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-73            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-74           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-75         [32, 50, 512]             1,024\n",
       "│    │    └─Decoderlayer: 3-10                [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-16     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-76            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-77            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-78            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-79            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-80         [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-17     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-81            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-82            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-83            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-84            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-85         [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-18     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-86            [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-87              [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-88            [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-89           [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-90         [32, 50, 512]             1,024\n",
       "│    │    └─Decoderlayer: 3-11                [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-19     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-91            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-92            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-93            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-94            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-95         [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-20     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-96            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-97            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-98            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-99            [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-100        [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-21     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-101           [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-102             [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-103           [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-104          [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-105        [32, 50, 512]             1,024\n",
       "│    │    └─Decoderlayer: 3-12                [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-22     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-106           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-107           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-108           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-109           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-110        [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-23     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-111           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-112           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-113           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-114           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-115        [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-24     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-116           [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-117             [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-118           [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-119          [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-120        [32, 50, 512]             1,024\n",
       "│    │    └─Decoderlayer: 3-13                [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-25     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-121           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-122           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-123           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-124           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-125        [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-26     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-126           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-127           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-128           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-129           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-130        [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-27     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-131           [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-132             [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-133           [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-134          [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-135        [32, 50, 512]             1,024\n",
       "│    │    └─Decoderlayer: 3-14                [32, 50, 512]             --\n",
       "│    │    │    └─MultiHeadAttention: 4-28     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-136           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-137           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-138           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-139           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-140        [32, 50, 512]             1,024\n",
       "│    │    │    └─MultiHeadAttention: 4-29     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-141           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-142           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-143           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─Linear: 5-144           [32, 50, 512]             262,144\n",
       "│    │    │    │    └─LayerNorm: 5-145        [32, 50, 512]             1,024\n",
       "│    │    │    └─FeedForwardNetwork: 4-30     [32, 50, 512]             --\n",
       "│    │    │    │    └─Linear: 5-146           [32, 50, 2048]            1,050,624\n",
       "│    │    │    │    └─ReLU: 5-147             [32, 50, 2048]            --\n",
       "│    │    │    │    └─Linear: 5-148           [32, 50, 512]             1,049,088\n",
       "│    │    │    │    └─Dropout: 5-149          [32, 50, 512]             --\n",
       "│    │    │    │    └─LayerNorm: 5-150        [32, 50, 512]             1,024\n",
       "├─Linear: 1-3                                 [32, 50, 1000]            512,000\n",
       "===============================================================================================\n",
       "Total params: 45,637,632\n",
       "Trainable params: 45,637,632\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.46\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 1087.59\n",
       "Params size (MB): 182.55\n",
       "Estimated Total Size (MB): 1270.17\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input = torch.randint(0, 1000, (1, 100)).to(device)\n",
    "decoder_input = torch.randint(0, 1000, (1, 100)).to(device)\n",
    "net = Transformer(d_model=d_model, max_len=max_len, d_ff=d_ff, d_k=d_k, d_v=d_v, n_layers=n_layers, n_heads=n_heads, p_drop=p_drop)\n",
    "print(net(encoder_input, decoder_input)[0].shape)\n",
    "# 创建一个样例输入来查看结构\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "\n",
    "summary(net, \n",
    "        input_size=[(batch_size, seq_len), (batch_size, seq_len)],\n",
    "        dtypes=[torch.long, torch.long],\n",
    "        device=device,\n",
    "        depth=5,  # 增加深度显示\n",
    "        )  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
